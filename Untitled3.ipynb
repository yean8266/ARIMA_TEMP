{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b4ca480-1be9-41e5-945b-986aad5b54ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功爬取数据：https://richurimo.bmcx.com/beijing__time__2021_01__richurimo/\n",
      "成功爬取数据：https://richurimo.bmcx.com/beijing__time__2021_02__richurimo/\n",
      "成功爬取数据：https://richurimo.bmcx.com/beijing__time__2021_03__richurimo/\n",
      "成功爬取数据：https://richurimo.bmcx.com/beijing__time__2021_04__richurimo/\n",
      "成功爬取数据：https://richurimo.bmcx.com/beijing__time__2021_05__richurimo/\n",
      "成功爬取数据：https://richurimo.bmcx.com/beijing__time__2021_06__richurimo/\n",
      "成功爬取数据：https://richurimo.bmcx.com/beijing__time__2021_07__richurimo/\n",
      "成功爬取数据：https://richurimo.bmcx.com/beijing__time__2021_08__richurimo/\n",
      "成功爬取数据：https://richurimo.bmcx.com/beijing__time__2021_09__richurimo/\n",
      "成功爬取数据：https://richurimo.bmcx.com/beijing__time__2021_10__richurimo/\n",
      "成功爬取数据：https://richurimo.bmcx.com/beijing__time__2021_11__richurimo/\n",
      "成功爬取数据：https://richurimo.bmcx.com/beijing__time__2021_12__richurimo/\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Author: W.Y.\n",
    "# Email: wangyingchn@outlook.com\n",
    "# Date: 2022/4/4\n",
    "\n",
    "# 导入模块\n",
    "import csv  # 用于存储数据\n",
    "import time  # 用于时间间隔避免过频繁的请求\n",
    "import requests  # 用于请求数据\n",
    "from bs4 import BeautifulSoup  # 用于解析数据\n",
    "\n",
    "# 请求数据\n",
    "def get_response(url):\n",
    "    # 伪装一下，让服务器以为是正常浏览，而不是爬虫。\n",
    "    # 静态网页通常反爬不严格，所以只要通过 User-Agent 伪装成浏览器即可。\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \\\n",
    "        (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36\"\n",
    "    }\n",
    "    # 请求数据。使用 get 方法请求数据\n",
    "    response = requests.get(url, headers=headers)\n",
    "    return response\n",
    "\n",
    "# 解析数据\n",
    "def parse_data(response):\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")  # 由于是通过 html 格式存储的，所以用 “html.parser” 进行解析\n",
    "    data_table = soup.find('table', attrs={'width':'100%','border':'0','cellpadding':'8','cellspacing':'1'}).find_all(\"tr\")  # 找到包裹表内容的 ul 标签，找到里面所有的 li 标签\n",
    "    weather_list = []  # 构造空列表以存储数据\n",
    "    for tr in data_table[1:]:  # 循环获取每行的数据（li 标签）\n",
    "        th_list = tr.find_all('td')  # 获取每行的每个数据 （li 标签下的 div 标签）\n",
    "        weather = {\n",
    "            '日照时数': th_list[4].get_text(),  # 获取第一个 div 标签中的内容，命名为 “date”\n",
    "        }   # 每行数据存储在一个字典中\n",
    "        weather_list.append(weather)  # 所有行的数据存入一个列表中\n",
    "    return weather_list\n",
    "# 储存数据\n",
    "def save_data(weather_list, save_path):\n",
    "    with open(save_path, 'a', newline='', encoding='utf-8') as fp:\n",
    "        csv_header = ['日照时数']  # 设置表头，即列名\n",
    "        csv_writer = csv.DictWriter(fp, csv_header)\n",
    "        if fp.tell() == 0:\n",
    "            csv_writer.writeheader()  # 如果文件不存在，则写入表头；如果文件已经存在，则直接追加数据不再次写入表头\n",
    "        csv_writer.writerows(weather_list)  # 写入数据\n",
    "\n",
    "# 构造网址\n",
    "def generate_urls():\n",
    "    url_pattern = 'https://richurimo.bmcx.com/beijing__time__{}__richurimo/'  # 网址的基本结构，有变化的两个部分用 {} 替代，后面循环补充\n",
    "    years = [2021]  # 使用列表生成式生成年份列表\n",
    "    months = [str(x).zfill(2) for x in range(1, 13)]  # 生成月份列表，zfill 函数补充两位数\n",
    "    month_list = [str(year) + '_' + str(month) for year in years for month in months]  # 年月循环拼在一起\n",
    "    url_list = []  # 空列表用于存储所有网址\n",
    "    for m in month_list:  # 再循环时间\n",
    "            url_list.append(url_pattern.format(m))  # 通过 format 函数生成网址\n",
    "    return url_list\n",
    "\n",
    "# 定义爬取函数\n",
    "def crawler(url, save_path):\n",
    "    response = get_response(url)    # 请求数据\n",
    "    results = parse_data(response)  # 解析数据\n",
    "    save_data(results, save_path)   # 存储数据\n",
    "    print(f'成功爬取数据：{url}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    urls = generate_urls()     # 构造所有网址\n",
    "    save_file = 'weather2.csv'  # 保存数据的文件路径\n",
    "    for u in urls:     # 在网址中循环\n",
    "        time.sleep(2)  # 每次爬取休息 2 秒，以免太过频繁的请求\n",
    "        crawler(u, save_file)  # 进行爬取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c1d6ba-547c-41f1-82ca-1d5c511e6131",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
