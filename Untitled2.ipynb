{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "189aa341-d47f-42e7-bcfe-a71f40eccbd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功爬取数据：https://lishi.tianqi.com/beijing/202101.html\n",
      "成功爬取数据：https://lishi.tianqi.com/beijing/202102.html\n",
      "成功爬取数据：https://lishi.tianqi.com/beijing/202103.html\n",
      "成功爬取数据：https://lishi.tianqi.com/beijing/202104.html\n",
      "成功爬取数据：https://lishi.tianqi.com/beijing/202105.html\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 76\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m u \u001b[38;5;129;01min\u001b[39;00m urls:     \u001b[38;5;66;03m# 在网址中循环\u001b[39;00m\n\u001b[0;32m     75\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# 每次爬取休息 2 秒，以免太过频繁的请求\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m     crawler(u, save_file)\n",
      "Cell \u001b[1;32mIn[8], line 67\u001b[0m, in \u001b[0;36mcrawler\u001b[1;34m(url, save_path)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcrawler\u001b[39m(url, save_path):\n\u001b[0;32m     66\u001b[0m     response \u001b[38;5;241m=\u001b[39m get_response(url)    \u001b[38;5;66;03m# 请求数据\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m     results \u001b[38;5;241m=\u001b[39m parse_data(response)  \u001b[38;5;66;03m# 解析数据\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     save_data(results, save_path)   \u001b[38;5;66;03m# 存储数据\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m成功爬取数据：\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 27\u001b[0m, in \u001b[0;36mparse_data\u001b[1;34m(response)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_data\u001b[39m(response):\n\u001b[0;32m     26\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# 由于是通过 html 格式存储的，所以用 “html.parser” 进行解析\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m     data_table \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mul\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthrui\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mli\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# 找到包裹表内容的 ul 标签，找到里面所有的 li 标签\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     weather_list \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m# 构造空列表以存储数据\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m li \u001b[38;5;129;01min\u001b[39;00m data_table[\u001b[38;5;241m0\u001b[39m:]:  \u001b[38;5;66;03m# 循环获取每行的数据（li 标签）\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Author: W.Y.\n",
    "# Email: wangyingchn@outlook.com\n",
    "# Date: 2022/4/4\n",
    "\n",
    "# 导入模块\n",
    "import csv  # 用于存储数据\n",
    "import time  # 用于时间间隔避免过频繁的请求\n",
    "import requests  # 用于请求数据\n",
    "from bs4 import BeautifulSoup  # 用于解析数据\n",
    "\n",
    "# 请求数据\n",
    "def get_response(url):\n",
    "    # 伪装一下，让服务器以为是正常浏览，而不是爬虫。\n",
    "    # 静态网页通常反爬不严格，所以只要通过 User-Agent 伪装成浏览器即可。\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \\\n",
    "        (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36\"\n",
    "    }\n",
    "    # 请求数据。使用 get 方法请求数据\n",
    "    response = requests.get(url, headers=headers)\n",
    "    return response\n",
    "\n",
    "# 解析数据\n",
    "def parse_data(response):\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")  # 由于是通过 html 格式存储的，所以用 “html.parser” 进行解析\n",
    "    data_table = soup.find('ul', class_=\"thrui\").find_all(\"li\")  # 找到包裹表内容的 ul 标签，找到里面所有的 li 标签\n",
    "    weather_list = []  # 构造空列表以存储数据\n",
    "    for li in data_table[0:]:  # 循环获取每行的数据（li 标签）\n",
    "        th_list = li.find_all('div')  # 获取每行的每个数据 （li 标签下的 div 标签）\n",
    "        weather = {\n",
    "            '日期': th_list[0].get_text(),  # 获取第一个 div 标签中的内容，命名为 “date”\n",
    "            '最高气温': th_list[1].get_text(),\n",
    "            '最低气温': th_list[2].get_text(),\n",
    "            '天气': th_list[3].get_text(),\n",
    "            '风向': th_list[4].get_text()[:-2],  # 获取风力，去掉最后两个字符\n",
    "            '风力': th_list[4].get_text()[-2:],  # 获取风向，保留最后两个字符\n",
    "            'url': response.url  # 爬取时通常可以顺便保存一下当页的网址，方便溯源和排查错误\n",
    "            # 'city': response.url.split('/')[3]  # 如果需要增加一列城市，也可以通过 url 来获取\n",
    "        }   # 每行数据存储在一个字典中\n",
    "        weather_list.append(weather)  # 所有行的数据存入一个列表中\n",
    "    return weather_list\n",
    "\n",
    "# 储存数据\n",
    "def save_data(weather_list, save_path):\n",
    "    with open(save_path, 'a', newline='', encoding='utf-8') as fp:\n",
    "        csv_header = ['日期', '最高气温', '最低气温', '天气', '风向','风力', 'url']  # 设置表头，即列名\n",
    "        csv_writer = csv.DictWriter(fp, csv_header)\n",
    "        if fp.tell() == 0:\n",
    "            csv_writer.writeheader()  # 如果文件不存在，则写入表头；如果文件已经存在，则直接追加数据不再次写入表头\n",
    "        csv_writer.writerows(weather_list)  # 写入数据\n",
    "\n",
    "# 构造网址\n",
    "def generate_urls():\n",
    "    url_pattern = 'https://lishi.tianqi.com/beijing/{}.html'  # 网址的基本结构，有变化的两个部分用 {} 替代，后面循环补充\n",
    "    years = [2021]  # 使用列表生成式生成年份列表\n",
    "    months = [str(x).zfill(2) for x in range(1, 13)]  # 生成月份列表，zfill 函数补充两位数\n",
    "    month_list = [str(year) + str(month) for year in years for month in months]  # 年月循环拼在一起\n",
    "    url_list = []  # 空列表用于存储所有网址\n",
    "    for m in month_list:  # 再循环时间\n",
    "            url_list.append(url_pattern.format(m))  # 通过 format 函数生成网址\n",
    "    return url_list\n",
    "\n",
    "# 定义爬取函数\n",
    "def crawler(url, save_path):\n",
    "    response = get_response(url)    # 请求数据\n",
    "    results = parse_data(response)  # 解析数据\n",
    "    save_data(results, save_path)   # 存储数据\n",
    "    print(f'成功爬取数据：{url}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    urls = generate_urls()     # 构造所有网址\n",
    "    save_file = 'weather.csv'  # 保存数据的文件路径\n",
    "    for u in urls:     # 在网址中循环\n",
    "        time.sleep(2)  # 每次爬取休息 2 秒，以免太过频繁的请求\n",
    "        crawler(u, save_file)  # 进行爬取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692ea5b4-8f58-43a5-a0a0-fca5ed4b8b00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
